%Statistics and data analysis. 
Analysis of data and the reporting of the results of those analyses are fundamental aspects of the conduct of research. Accurate, unbiased, complete, and insightful reporting of the analytic treatment of data (be it quantitative or qualitative) must be a component of all research reports. Researchers in the field of psychology use numerous approaches to the analysis of data, and no one approach is uniformly preferred as long as the method is appropriate to the research questions being asked and the nature of the data collected. The methods used must support their analytic burdens, including robustness to violations of the assumptions that underlie them, and they must provide clear, unequivocal insights into the data.

Historically, researchers in psychology have relied heavily on null hypothesis statistical significance testing (NHST) as a starting point for many (but not all) of its analytic approaches. APA stresses that NHST is but a starting point and that additional reporting elements such as effect sizes, confidence intervals, and extensive description are needed to convey the most complete meaning of the results. The degree to which any journal emphasizes (or de-emphasizes) NHST is a decision of the individual editor. However, complete reporting of all tested hypotheses and estimates of appropriate effect sizes and confidence intervals are the minimum expectations for all APA journals. The research scientist is always responsible for the accurate and responsible in reporting of the results of research studies.

Assume that your reader has a professional knowledge of statistical methods. Do not review basic concepts and procedures or provide citations for the most commonly used statistical procedures. If, however, there is any question about the appropriateness of a particular statistical procedure, justify its use by clearly stating the evidence that exists for the robustness of the procedure as applied.

Similarly, missing data can have a detrimental effect on the legitimacy of the inferences drawn by statistical tests. For this reason, it is critical that the frequency or percentages of missing data be reported along with any empirical evidence and/or theoretical arguments for the causes of data that are missing. For example, data might be described as missing completely at random (as when values of the missing variable are not related to the probability that they are missing or to the value of any other variable in the data set); missing at random (as when the probability of missing a value on a variable is not related to the missing value itself but may be related to other completely observed variables in the data set); or not missing at random (as when the probability of observing a given value for a variable is related to the missing value itself). It is also important to describe the methods for addressing missing data, if any were used (e.g., multiple imputation).

When reporting the results of inferential statistical tests or when providing estimates of parameters or effect sizes, include sufficient information to help the reader fully understand the analyses conducted and possible alternative explanations for the outcomes of those analyses. Because each analytic technique depends on different aspects of the data and assumptions, it is impossible to specify what constitutes a ``sufficient set of statistics'' for every analysis. However, such a set usually includes at least the following: the per-cell sample sizes; the observed cell means (or frequencies of cases in each category for a categorical variable); and the cell standard deviations, or the pooled within-cell variance. In the case of multivariable analytic systems, such as multivariate analyses of variance, regression analyses, structural equation modeling analyses, and hierarchical linear modeling, the associated means, sample sizes, and variance—covariance (or correlation) matrix or matrices often represent a sufficient set of statistics. At times, the amount of information that constitutes a sufficient set of statistics can be extensive; when this is the case, this information could be supplied in a supplementary data set or appendix. For analyses based on very small samples (including single-case investigations), consider providing the complete set of raw data in a table or figure. Your work will more easily become a part of the cumulative knowledge of the field if you include enough statistical information to allow its inclusion in future meta-analyses.

For inferential statistical tests (e.g., $t$, $F$, and $\chi^{2}$ tests), include the obtained magnitude or value of the test statistic, the degrees of freedom, the probability of obtaining a value as extreme as or more extreme than the one obtained (the exact p value), and the size and direction of the effect. When point estimates (e.g., sample means or regression coefficients) are provided, always include an associated measure of variability(precision), with an indication of the specific measure used (e.g., the standard error).

The inclusion of confidence intervals (for estimates of parameters, for functions of parameters such as differences in means, and for effect sizes) can be an extremely effective way of reporting results. Because confidence intervals combine information on location and precision and can often be directly used to infer significance levels, they are, in general, the best reporting strategy. The use of confidence intervals is therefore strongly recommended. As a rule, it is best to use a single confidence level, specified on an a priori basis (e.g., a 95\% or 99\% confidence interval), throughout the manuscript. Wherever possible, base discussion and interpretation of results on point and interval estimates.

For the reader to appreciate the magnitude or importance of a study's findings, it is almost always necessary to include some measure of effect size in the Results section. Whenever possible, provide a confidence interval for each effect size reported to indicate the precision of estimation of the effect size. Effect sizes may be expressed in the original units (e.g., the mean number of questions answered correctly; kg/month for a regression slope) and are often most easily understood when reported in original units. It can often be valuable to report an effect size not only in original units but also in some standardized or units-free unit (e.g., as a Cohen's $d$ value) or a standardized regression weight. Multiple degree-of-freedom effect-size indicators are often less useful than effect-size indicators that decompose multiple degree-of-freedom tests into meaningful one degree-of-freedom effects—particularly when the latter are the results that inform the discussion. The general principle to be followed, however, is to provide the reader with enough information to assess the magnitude of the observed effect.